{%- from "kubernetes/map.jinja" import master with context %}
{%- from "kubernetes/map.jinja" import pool with context %}

{%- set network = {} %}
{%- if pool.get('enabled', False) %}
{%- set network = pool.get('network', {}) %}
{%- elif master.get('enabled', False) %}
{%- set network = master.get('network', {}) %}
{%- endif %}

server:
{%- if network.get('engine', '') == 'calico' and network.get('prometheus', {}).get('enabled', False) %}
  target:
    kubernetes:
      enabled: true
      api_ip: {{ pool.apiserver.host }}
      cert_name: prometheus-server.crt
      key_name: prometheus-server.key
    static:
      calico:
        endpoint:
{%- if pool.get('enabled', False) %}
          - address: {{ network.prometheus.get('address', pool.address) }}
{%- else %}
          - address: {{ network.prometheus.get('address', master.address) }}
{%- endif %}
            port: {{ network.prometheus.get('port', 9091) }}
{%- endif %}
  recording:
    cluster_namespace_controller_pod_container:spec_memory_limit_bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_memory_limit_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:spec_cpu_shares:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_cpu_shares{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:cpu_usage:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            irate(
              container_cpu_usage_seconds_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_usage:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_working_set:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_working_set_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_rss:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_rss{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_cache:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_cache{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:disk_usage:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_fs_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_pagefaults:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failures_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_oom:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failcnt{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster:memory_allocation:percent:
      query: >-
        100 * sum by (cluster) (
          container_spec_memory_limit_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )
    cluster:memory_used:percent:
      query: >-
        100 * sum by (cluster) (
          container_memory_usage_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )
    cluster:cpu_allocation:percent:
      query: >-
        100 * sum by (cluster) (
          container_spec_cpu_shares{pod_name!=""}
        ) / sum by (cluster) (
          container_spec_cpu_shares{id="/"} * on(cluster,instance) machine_cpu_cores
        )
  alert:
    AvgKubeletRunningContainerCountLow:
      if: >-
        avg_over_time(kubelet_running_container_count[2m]) * 1.3 <
        avg_over_time(kubelet_running_container_count[10m])
      {% raw %}
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: 'Container count is low'
        description: 'Container count from last 2m is lower than avarage from 10m'
      {% endraw %}
    AvgKubeletRunningPODCountLow:
      if: >-
        avg_over_time(kubelet_running_pod_count[2m]) * 1.3 <
        avg_over_time(kubelet_running_pod_count[10m])
      {% raw %}
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: 'POD count is low'
        description: 'POD count from last 2m is lower than avarage from 10m'
      {% endraw %}
    ContainerScrapeError:
      if: 'container_scrape_error != 0'
      {% raw %}
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: 'Fail to scrape container'
        description: 'Prometheus was not able to scrape metrics from container on {{ $labels.instance }}'
      {% endraw %}
    KubernetesProcessDown:
      if: >-
        procstat_running{process_name=~"hypercube-.*"} == 0
      {% raw %}
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: 'Kubernetes service {{ $labels.process_name }} is down'
        description: 'Kubernetes service {{ $labels.process_name }} is down on node {{ $labels.host }}'
      {% endraw %}
{%- if network.get('engine', '') == 'calico' %}
    CalicoProcessDown:
      if: >-
        procstat_running{process_name=~"calico-felix|bird|bird6|confd"} == 0
      {% raw %}
      labels:
        severity: warning
        service: calico
      annotations:
        summary: 'Calico service {{ $labels.process_name }} is down'
        description: 'Calico service {{ $labels.process_name }} is down on node {{ $labels.host }}'
      {% endraw %}
{% endif %}
